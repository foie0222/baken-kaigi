# LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…è¨ˆç”»: è²·ã„ç›®ææ¡ˆã®æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** è²·ã„ç›®ææ¡ˆã®æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆï¼ˆproposal_reasoningï¼‰ã‚’ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆã‹ã‚‰Haiku 4.5ã«ã‚ˆã‚‹LLMè‡ªç„¶è¨€èªç”Ÿæˆã«ç½®æ›ã™ã‚‹

**Architecture:** `_generate_proposal_reasoning()` ã®å†…éƒ¨ã‚’3å±¤ã«åˆ†å‰²ã€‚`_build_narration_context()` ã§ãƒ‡ãƒ¼ã‚¿æ•´ç† â†’ `_invoke_haiku_narrator()` ã§Bedrock APIå‘¼ã³å‡ºã— â†’ å¤±æ•—æ™‚ã¯æ—¢å­˜ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚Phase 0-6ã®ãƒ­ã‚¸ãƒƒã‚¯ã€ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ä¸€åˆ‡å¤‰æ›´ã—ãªã„ã€‚

**Tech Stack:** Python, Strands Agents SDK, Amazon Bedrock (Haiku 4.5), boto3, pytest

---

### Task 1: `_build_narration_context()` ã®ãƒ†ã‚¹ãƒˆã‚’æ›¸ã

**Files:**
- Test: `backend/tests/agentcore/test_bet_proposal.py`

**Step 1: ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã¨ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ **

`test_bet_proposal.py` ã®æœ«å°¾ï¼ˆ`TestProposalReasoningInImpl` ã‚¯ãƒ©ã‚¹ã®å¾Œï¼‰ã«è¿½åŠ :

```python
class TestBuildNarrationContext:
    """_build_narration_context ã®ãƒ†ã‚¹ãƒˆ."""

    def _make_reasoning_args(self):
        """TestProposalReasoning ã¨åŒã˜ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿."""
        runners = _make_runners(6)
        ai_preds = _make_ai_predictions(6)
        axis_horses = [
            {"horse_number": 1, "horse_name": "ãƒ†ã‚¹ãƒˆé¦¬1", "composite_score": 85.0},
            {"horse_number": 2, "horse_name": "ãƒ†ã‚¹ãƒˆé¦¬2", "composite_score": 72.0},
        ]
        difficulty = {"difficulty_stars": 3, "difficulty_label": "æ¨™æº–"}
        skip = {"skip_score": 3, "reasons": [], "recommendation": "å‚æˆ¦æ¨å¥¨"}
        bets = [
            {
                "bet_type": "quinella", "bet_type_name": "é¦¬é€£",
                "horse_numbers": [1, 3], "expected_value": 1.8,
                "composite_odds": 8.5, "confidence": "high",
            },
        ]
        return dict(
            axis_horses=axis_horses,
            difficulty=difficulty,
            predicted_pace="ãƒŸãƒ‰ãƒ«",
            ai_consensus="æ¦‚ã­åˆæ„",
            skip=skip,
            bets=bets,
            preferred_bet_types=None,
            ai_predictions=ai_preds,
            runners_data=runners,
        )

    def test_å¿…é ˆã‚­ãƒ¼ãŒå…¨ã¦å«ã¾ã‚Œã‚‹(self):
        """context dictã«å¿…è¦ãªã‚­ãƒ¼ãŒå…¨ã¦å­˜åœ¨ã™ã‚‹."""
        args = self._make_reasoning_args()
        ctx = _build_narration_context(**args)
        required_keys = {
            "axis_horses", "partner_horses", "difficulty",
            "predicted_pace", "ai_consensus", "skip", "bets",
        }
        assert required_keys.issubset(ctx.keys())

    def test_è»¸é¦¬ã«AIé †ä½ã¨ã‚¹ã‚³ã‚¢ãŒä»˜ä¸ã•ã‚Œã‚‹(self):
        """axis_horses ã®å„è¦ç´ ã« ai_rank, ai_score ãŒå«ã¾ã‚Œã‚‹."""
        args = self._make_reasoning_args()
        ctx = _build_narration_context(**args)
        for horse in ctx["axis_horses"]:
            assert "ai_rank" in horse
            assert "ai_score" in horse
            assert isinstance(horse["ai_rank"], int)
            assert isinstance(horse["ai_score"], float)

    def test_ç›¸æ‰‹é¦¬ãŒæŠ½å‡ºã•ã‚Œã‚‹(self):
        """betsã‹ã‚‰è»¸é¦¬ä»¥å¤–ã®é¦¬ç•ªãŒ partner_horses ã«å«ã¾ã‚Œã‚‹."""
        args = self._make_reasoning_args()
        ctx = _build_narration_context(**args)
        assert len(ctx["partner_horses"]) > 0
        partner_numbers = {p["horse_number"] for p in ctx["partner_horses"]}
        axis_numbers = {1, 2}
        assert partner_numbers.isdisjoint(axis_numbers)

    def test_ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹(self):
        """speed_index_data ãŒæ¸¡ã•ã‚ŒãŸå ´åˆã€context ã« speed_index_raw ãŒå«ã¾ã‚Œã‚‹."""
        args = self._make_reasoning_args()
        args["speed_index_data"] = {
            "horses": {1: {"indices": [80, 85], "avg": 82.5}},
        }
        ctx = _build_narration_context(**args)
        assert "speed_index_raw" in ctx

    def test_ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ãªã—ã®å ´åˆã¯ã‚­ãƒ¼ãŒå­˜åœ¨ã—ãªã„(self):
        """speed_index_data ãŒ None ã®å ´åˆã€speed_index_raw ã¯å«ã¾ã‚Œãªã„."""
        args = self._make_reasoning_args()
        args["speed_index_data"] = None
        ctx = _build_narration_context(**args)
        assert "speed_index_raw" not in ctx

    def test_éå»æˆç¸¾ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã‚‹(self):
        """past_performance_data ãŒæ¸¡ã•ã‚ŒãŸå ´åˆã€context ã« past_performance_raw ãŒå«ã¾ã‚Œã‚‹."""
        args = self._make_reasoning_args()
        args["past_performance_data"] = {
            "horses": {1: {"results": [1, 3, 2]}},
        }
        ctx = _build_narration_context(**args)
        assert "past_performance_raw" in ctx

    def test_Decimalå‹ãƒ‡ãƒ¼ã‚¿ã§ã‚‚ã‚¨ãƒ©ãƒ¼ã«ãªã‚‰ãªã„(self):
        """DynamoDB Decimal å‹ã§ã‚‚æ­£å¸¸å‹•ä½œã™ã‚‹."""
        args = self._make_reasoning_args()
        for pred in args["ai_predictions"]:
            pred["horse_number"] = Decimal(str(pred["horse_number"]))
            pred["rank"] = Decimal(str(pred["rank"]))
            pred["score"] = Decimal(str(pred["score"]))
        ctx = _build_narration_context(**args)
        assert len(ctx["axis_horses"]) == 2
```

**Step 2: import ã« `_build_narration_context` ã‚’è¿½åŠ **

`test_bet_proposal.py` ã®å…ˆé ­ã«ã‚ã‚‹importï¼ˆL24ä»˜è¿‘ï¼‰ã«è¿½åŠ :

```python
from tools.bet_proposal import (
    ...
    _build_narration_context,
    ...
)
```

**Step 3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¦å¤±æ•—ã‚’ç¢ºèª**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py::TestBuildNarrationContext -v`
Expected: FAILï¼ˆImportError: `_build_narration_context` ãŒå­˜åœ¨ã—ãªã„ï¼‰

**Step 4: ã‚³ãƒŸãƒƒãƒˆ**

```bash
git add backend/tests/agentcore/test_bet_proposal.py
git commit -m "test: _build_narration_context ã®å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ "
```

---

### Task 2: `_build_narration_context()` ã‚’å®Ÿè£…ã™ã‚‹

**Files:**
- Modify: `backend/agentcore/tools/bet_proposal.py`ï¼ˆ`_generate_proposal_reasoning` ã®ç›´å‰ã€L1604ä»˜è¿‘ã«è¿½åŠ ï¼‰

**Step 1: é–¢æ•°ã‚’å®Ÿè£…**

`_generate_proposal_reasoning()` ã®ç›´å‰ã«è¿½åŠ :

```python
def _build_narration_context(
    axis_horses: list[dict],
    difficulty: dict,
    predicted_pace: str,
    ai_consensus: str,
    skip: dict,
    bets: list[dict],
    preferred_bet_types: list[str] | None,
    ai_predictions: list[dict],
    runners_data: list[dict],
    speed_index_data: dict | None = None,
    past_performance_data: dict | None = None,
) -> dict:
    """LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆdictã‚’æ§‹ç¯‰ã™ã‚‹."""
    # AIé †ä½ãƒ»ã‚¹ã‚³ã‚¢ãƒãƒƒãƒ—ï¼ˆDecimalå¯¾ç­–ï¼‰
    ai_rank_map = {
        int(p.get("horse_number", 0)): int(p.get("rank", 99))
        for p in ai_predictions
    }
    ai_score_map = {
        int(p.get("horse_number", 0)): float(p.get("score", 0))
        for p in ai_predictions
    }
    runners_map = {r.get("horse_number"): r for r in runners_data}

    # è»¸é¦¬ã«AIæƒ…å ±ã‚’ä»˜ä¸
    enriched_axis = []
    for ax in axis_horses:
        hn = ax["horse_number"]
        runner = runners_map.get(hn, {})
        enriched = {
            "horse_number": hn,
            "horse_name": ax.get("horse_name", ""),
            "composite_score": float(ax.get("composite_score", 0)),
            "ai_rank": ai_rank_map.get(hn, 99),
            "ai_score": ai_score_map.get(hn, 0),
            "odds": float(runner.get("odds", 0)) if runner.get("odds") else 0,
        }
        if speed_index_data:
            si_score = _calculate_speed_index_score(hn, speed_index_data)
            if si_score is not None:
                enriched["speed_index_score"] = float(si_score)
        if past_performance_data:
            form_s = _calculate_form_score(hn, past_performance_data)
            if form_s is not None:
                enriched["form_score"] = float(form_s)
        enriched_axis.append(enriched)

    # ç›¸æ‰‹é¦¬ã‚’æŠ½å‡º
    axis_numbers = {ax["horse_number"] for ax in axis_horses}
    partner_numbers_seen = []
    for bet in bets:
        for hn in bet.get("horse_numbers", []):
            if hn not in axis_numbers and hn not in partner_numbers_seen:
                partner_numbers_seen.append(hn)

    partners = []
    for hn in partner_numbers_seen[:MAX_PARTNERS]:
        runner = runners_map.get(hn, {})
        ev_vals = [
            b.get("expected_value", 0) for b in bets
            if hn in b.get("horse_numbers", [])
        ]
        partners.append({
            "horse_number": hn,
            "horse_name": runner.get("horse_name", ""),
            "ai_rank": ai_rank_map.get(hn, 99),
            "max_expected_value": max(ev_vals) if ev_vals else 0,
        })

    ctx = {
        "axis_horses": enriched_axis,
        "partner_horses": partners,
        "difficulty": difficulty,
        "predicted_pace": predicted_pace,
        "ai_consensus": ai_consensus,
        "skip": skip,
        "bets": [
            {
                "bet_type_name": b.get("bet_type_name", ""),
                "horse_numbers": b.get("horse_numbers", []),
                "expected_value": b.get("expected_value", 0),
                "composite_odds": float(b.get("composite_odds", 0)),
                "confidence": b.get("confidence", ""),
            }
            for b in bets
        ],
    }
    if preferred_bet_types:
        ctx["preferred_bet_types"] = preferred_bet_types
    if speed_index_data:
        ctx["speed_index_raw"] = speed_index_data
    if past_performance_data:
        ctx["past_performance_raw"] = past_performance_data
    return ctx
```

**Step 2: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¦æˆåŠŸã‚’ç¢ºèª**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py::TestBuildNarrationContext -v`
Expected: ALL PASS

**Step 3: æ—¢å­˜ãƒ†ã‚¹ãƒˆãŒå£Šã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py -v --tb=short 2>&1 | tail -20`
Expected: æ—¢å­˜ãƒ†ã‚¹ãƒˆå…¨ã¦PASS

**Step 4: ã‚³ãƒŸãƒƒãƒˆ**

```bash
git add backend/agentcore/tools/bet_proposal.py backend/tests/agentcore/test_bet_proposal.py
git commit -m "feat: _build_narration_context() ã‚’å®Ÿè£…"
```

---

### Task 3: `_invoke_haiku_narrator()` ã®ãƒ†ã‚¹ãƒˆã‚’æ›¸ã

**Files:**
- Test: `backend/tests/agentcore/test_bet_proposal.py`

**Step 1: ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ **

```python
class TestInvokeHaikuNarrator:
    """_invoke_haiku_narrator ã®ãƒ†ã‚¹ãƒˆ."""

    def _make_context(self):
        """æœ€å°é™ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ."""
        return {
            "axis_horses": [
                {
                    "horse_number": 1, "horse_name": "ãƒ†ã‚¹ãƒˆé¦¬1",
                    "composite_score": 85.0, "ai_rank": 1, "ai_score": 500,
                    "odds": 3.4,
                },
            ],
            "partner_horses": [
                {"horse_number": 3, "horse_name": "ãƒ†ã‚¹ãƒˆé¦¬3", "ai_rank": 2, "max_expected_value": 1.5},
            ],
            "difficulty": {"difficulty_stars": 3, "difficulty_label": "æ¨™æº–"},
            "predicted_pace": "ãƒŸãƒ‰ãƒ«",
            "ai_consensus": "æ¦‚ã­åˆæ„",
            "skip": {"skip_score": 2, "reasons": [], "recommendation": "å‚æˆ¦æ¨å¥¨"},
            "bets": [
                {
                    "bet_type_name": "é¦¬é€£", "horse_numbers": [1, 3],
                    "expected_value": 1.5, "composite_odds": 8.5, "confidence": "high",
                },
            ],
        }

    @patch("tools.bet_proposal._call_bedrock_haiku")
    def test_æ­£å¸¸æ™‚ã«LLMç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™(self, mock_call):
        """Bedrockæ­£å¸¸æ™‚ã¯LLMç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™."""
        mock_call.return_value = (
            "ã€è»¸é¦¬é¸å®šã€‘1ç•ªãƒ†ã‚¹ãƒˆé¦¬1ã‚’è»¸ã«ã€‚AIæŒ‡æ•°1ä½ã§ä¿¡é ¼åº¦ãŒé«˜ã„\n\n"
            "ã€åˆ¸ç¨®ã€‘ãƒ¬ãƒ¼ã‚¹é›£æ˜“åº¦â˜…3ã®ãŸã‚é¦¬é€£ã‚’é¸å®š\n\n"
            "ã€çµ„ã¿åˆã‚ã›ã€‘ç›¸æ‰‹ã¯3ç•ªãƒ†ã‚¹ãƒˆé¦¬3ã€‚æœŸå¾…å€¤1.5ã§å¦™å‘³ã‚ã‚Š\n\n"
            "ã€ãƒªã‚¹ã‚¯ã€‘AIåˆè­°ã€Œæ¦‚ã­åˆæ„ã€ã€‚è¦‹é€ã‚Šã‚¹ã‚³ã‚¢2/10ã§ç©æ¥µå‚æˆ¦ãƒ¬ãƒ™ãƒ«"
        )
        result = _invoke_haiku_narrator(self._make_context())
        assert "ã€è»¸é¦¬é¸å®šã€‘" in result
        assert "ã€åˆ¸ç¨®ã€‘" in result
        assert "ã€çµ„ã¿åˆã‚ã›ã€‘" in result
        assert "ã€ãƒªã‚¹ã‚¯ã€‘" in result
        mock_call.assert_called_once()

    @patch("tools.bet_proposal._call_bedrock_haiku")
    def test_LLMãŒ4ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¿”ã•ãªã„å ´åˆã¯Noneã‚’è¿”ã™(self, mock_call):
        """LLMãŒä¸å®Œå…¨ãªå‡ºåŠ›ã‚’ã—ãŸå ´åˆã¯Noneï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰."""
        mock_call.return_value = "ä¸å®Œå…¨ãªå›ç­”ã§ã™"
        result = _invoke_haiku_narrator(self._make_context())
        assert result is None

    @patch("tools.bet_proposal._call_bedrock_haiku")
    def test_APIä¾‹å¤–æ™‚ã¯Noneã‚’è¿”ã™(self, mock_call):
        """Bedrock APIã‚¨ãƒ©ãƒ¼æ™‚ã¯Noneï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰."""
        mock_call.side_effect = Exception("ServiceUnavailable")
        result = _invoke_haiku_narrator(self._make_context())
        assert result is None
```

**Step 2: import ã‚’è¿½åŠ **

```python
from unittest.mock import patch
from tools.bet_proposal import (
    ...
    _invoke_haiku_narrator,
    ...
)
```

**Step 3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¦å¤±æ•—ã‚’ç¢ºèª**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py::TestInvokeHaikuNarrator -v`
Expected: FAILï¼ˆImportError: `_invoke_haiku_narrator` ãŒå­˜åœ¨ã—ãªã„ï¼‰

**Step 4: ã‚³ãƒŸãƒƒãƒˆ**

```bash
git add backend/tests/agentcore/test_bet_proposal.py
git commit -m "test: _invoke_haiku_narrator ã®å¤±æ•—ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ "
```

---

### Task 4: `_call_bedrock_haiku()` ã¨ `_invoke_haiku_narrator()` ã‚’å®Ÿè£…ã™ã‚‹

**Files:**
- Modify: `backend/agentcore/tools/bet_proposal.py`

**Step 1: ãƒ•ã‚¡ã‚¤ãƒ«å…ˆé ­ã® import ã« `json`, `logging`, `boto3` ã‚’è¿½åŠ **

`bet_proposal.py` ã® import ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆL7ä»˜è¿‘ï¼‰ã«è¿½åŠ :

```python
import json
import logging
import math

import boto3
import requests
```

**Step 2: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨å®šæ•°ã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ **

å®šæ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ`MAX_AXIS_HORSES` ç­‰ã®å®šç¾©ã®å¾Œã€L87ä»˜è¿‘ï¼‰ã«è¿½åŠ :

```python
# LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ãƒ¢ãƒ‡ãƒ«ID
NARRATOR_MODEL_ID = "jp.anthropic.claude-haiku-4-5-20251001-v1:0"

# LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
NARRATOR_SYSTEM_PROMPT = """ã‚ãªãŸã¯ç«¶é¦¬ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã«ã€è²·ã„ç›®ææ¡ˆã®æ ¹æ‹ ã‚’4ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ›¸ã„ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå³å®ˆï¼‰
ä»¥ä¸‹ã®4ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ”¹è¡ŒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã¯ã€ã€‘ã§å›²ã‚€ã€‚
å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä»¥å¤–ã®ãƒ†ã‚­ã‚¹ãƒˆã¯å‡ºåŠ›ã—ãªã„ã“ã¨ã€‚

ã€è»¸é¦¬é¸å®šã€‘...

ã€åˆ¸ç¨®ã€‘...

ã€çµ„ã¿åˆã‚ã›ã€‘...

ã€ãƒªã‚¹ã‚¯ã€‘...

## ãƒ«ãƒ¼ãƒ«
- 4ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆã€è»¸é¦¬é¸å®šã€‘ã€åˆ¸ç¨®ã€‘ã€çµ„ã¿åˆã‚ã›ã€‘ã€ãƒªã‚¹ã‚¯ã€‘ï¼‰ã¯å¿…é ˆ
- å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³1ã€œ3æ–‡ã§ç°¡æ½”ã«
- ãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤ï¼ˆAIæŒ‡æ•°é †ä½ãƒ»ã‚¹ã‚³ã‚¢ãƒ»ã‚ªãƒƒã‚ºç­‰ï¼‰ã¯æ­£ç¢ºã«å¼•ç”¨ã™ã‚‹ã“ã¨
- ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ç‰¹å¾´ã‚„æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆã‚’è‡ªåˆ†ã®è¨€è‘‰ã§è§£èª¬ã™ã‚‹ã“ã¨
- éå»æˆç¸¾ãŒã‚ã‚‹å ´åˆã¯ã€å…·ä½“çš„ãªç€é †æ¨ç§»ã‚„è·é›¢é©æ€§ã«è¨€åŠ
- ã‚¹ãƒ”ãƒ¼ãƒ‰æŒ‡æ•°ãŒã‚ã‚‹å ´åˆã¯ã€æŒ‡æ•°ã®ä½ç½®ã¥ã‘ã‚„æ¨ç§»ã«è¨€åŠ
- ã€ŒãŠã™ã™ã‚ã€ã€Œè²·ã†ã¹ãã€ç­‰ã®æ¨å¥¨è¡¨ç¾ã¯ç¦æ­¢

## ãƒˆãƒ¼ãƒ³åˆ¶å¾¡
- AIåˆè­°ãŒã€Œæ˜ç¢ºãªä¸Šä½ã€ã€Œæ¦‚ã­åˆæ„ã€â†’ ç¢ºä¿¡çš„ã«èªã‚‹
- AIåˆè­°ãŒã€Œã‚„ã‚„æ¥æˆ¦ã€ã€Œæ··æˆ¦ã€â†’ æ…é‡ã«ã€ãƒªã‚¹ã‚¯ã«ã‚‚è§¦ã‚ŒãªãŒã‚‰èªã‚‹
- è¦‹é€ã‚Šã‚¹ã‚³ã‚¢â‰¥7 â†’ è­¦æˆ’çš„ã«ã€äºˆç®—å‰Šæ¸›ã‚’å¼·èª¿"""

logger = logging.getLogger(__name__)
```

**Step 3: `_call_bedrock_haiku()` ã‚’å®Ÿè£…**

`_build_narration_context()` ã®ç›´å¾Œã«è¿½åŠ :

```python
def _call_bedrock_haiku(system_prompt: str, user_message: str) -> str:
    """Bedrock Converse API ã§ Haiku ã‚’å‘¼ã³å‡ºã™."""
    client = boto3.client("bedrock-runtime", region_name="ap-northeast-1")
    response = client.converse(
        modelId=NARRATOR_MODEL_ID,
        system=[{"text": system_prompt}],
        messages=[{"role": "user", "content": [{"text": user_message}]}],
        inferenceConfig={"maxTokens": 1024, "temperature": 0.7},
    )
    return response["output"]["message"]["content"][0]["text"]
```

**Step 4: `_invoke_haiku_narrator()` ã‚’å®Ÿè£…**

```python
def _invoke_haiku_narrator(context: dict) -> str | None:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…ƒã«Haikuã§ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹.

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚4ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒã‚ãªã„å ´åˆã‚„ã‚¨ãƒ©ãƒ¼æ™‚ã¯ Noneã€‚
    """
    try:
        user_message = json.dumps(context, ensure_ascii=False, default=str)
        text = _call_bedrock_haiku(NARRATOR_SYSTEM_PROMPT, user_message)
        # 4ã‚»ã‚¯ã‚·ãƒ§ãƒ³å…¨ã¦å«ã¾ã‚Œã‚‹ã‹æ¤œè¨¼
        required = ["ã€è»¸é¦¬é¸å®šã€‘", "ã€åˆ¸ç¨®ã€‘", "ã€çµ„ã¿åˆã‚ã›ã€‘", "ã€ãƒªã‚¹ã‚¯ã€‘"]
        if all(section in text for section in required):
            return text.strip()
        logger.warning("LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: 4ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸è¶³ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸ã€‚")
        return None
    except Exception:
        logger.exception("LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: Bedrockå‘¼ã³å‡ºã—å¤±æ•—ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¸ã€‚")
        return None
```

**Step 5: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¦æˆåŠŸã‚’ç¢ºèª**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py::TestInvokeHaikuNarrator -v`
Expected: ALL PASS

**Step 6: ã‚³ãƒŸãƒƒãƒˆ**

```bash
git add backend/agentcore/tools/bet_proposal.py
git commit -m "feat: _call_bedrock_haiku() ã¨ _invoke_haiku_narrator() ã‚’å®Ÿè£…"
```

---

### Task 5: `_generate_proposal_reasoning()` ã‚’LLMå‘¼ã³å‡ºã—ã«ç½®æ›ã™ã‚‹

**Files:**
- Modify: `backend/agentcore/tools/bet_proposal.py`ï¼ˆL1604-1683ã® `_generate_proposal_reasoning()` ã‚’å¤‰æ›´ï¼‰

**Step 1: æ—¢å­˜é–¢æ•°ã‚’ãƒªãƒãƒ¼ãƒ **

æ—¢å­˜ã® `_generate_proposal_reasoning()` ã‚’ `_generate_proposal_reasoning_template()` ã«ãƒªãƒãƒ¼ãƒ ã€‚

**Step 2: æ–°ã—ã„ `_generate_proposal_reasoning()` ã‚’å®Ÿè£…**

ãƒªãƒãƒ¼ãƒ å¾Œã®é–¢æ•°ã®ç›´å¾Œã«ã€åŒã˜ã‚·ã‚°ãƒãƒãƒ£ã§æ–°é–¢æ•°ã‚’è¿½åŠ :

```python
def _generate_proposal_reasoning(
    axis_horses: list[dict],
    difficulty: dict,
    predicted_pace: str,
    ai_consensus: str,
    skip: dict,
    bets: list[dict],
    preferred_bet_types: list[str] | None,
    ai_predictions: list[dict],
    runners_data: list[dict],
    skip_gate_threshold: int = SKIP_GATE_THRESHOLD,
    speed_index_data: dict | None = None,
    past_performance_data: dict | None = None,
) -> str:
    """ææ¡ˆæ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆã‚’4ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ç”Ÿæˆã™ã‚‹ï¼ˆLLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆï¼‰."""
    context = _build_narration_context(
        axis_horses=axis_horses,
        difficulty=difficulty,
        predicted_pace=predicted_pace,
        ai_consensus=ai_consensus,
        skip=skip,
        bets=bets,
        preferred_bet_types=preferred_bet_types,
        ai_predictions=ai_predictions,
        runners_data=runners_data,
        speed_index_data=speed_index_data,
        past_performance_data=past_performance_data,
    )
    result = _invoke_haiku_narrator(context)
    if result is not None:
        return result
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆ
    return _generate_proposal_reasoning_template(
        axis_horses=axis_horses,
        difficulty=difficulty,
        predicted_pace=predicted_pace,
        ai_consensus=ai_consensus,
        skip=skip,
        bets=bets,
        preferred_bet_types=preferred_bet_types,
        ai_predictions=ai_predictions,
        runners_data=runners_data,
        skip_gate_threshold=skip_gate_threshold,
        speed_index_data=speed_index_data,
        past_performance_data=past_performance_data,
    )
```

**Step 3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py -v --tb=short 2>&1 | tail -30`
Expected: æ—¢å­˜ãƒ†ã‚¹ãƒˆã®ä¸€éƒ¨ãŒå¤±æ•—ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Šï¼ˆBedrockå‘¼ã³å‡ºã—ã®ãŸã‚ï¼‰â†’ Task 6 ã§å¯¾å‡¦

**Step 4: ã‚³ãƒŸãƒƒãƒˆ**

```bash
git add backend/agentcore/tools/bet_proposal.py
git commit -m "feat: _generate_proposal_reasoning ã‚’LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç‰ˆã«ç½®æ›"
```

---

### Task 6: æ—¢å­˜ãƒ†ã‚¹ãƒˆã‚’LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œã«æ›´æ–°ã™ã‚‹

**Files:**
- Modify: `backend/tests/agentcore/test_bet_proposal.py`

**Step 1: `TestProposalReasoning` ã®ãƒ†ã‚¹ãƒˆã« Bedrock ãƒ¢ãƒƒã‚¯ã‚’é©ç”¨**

æ—¢å­˜ã® `TestProposalReasoning` ã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆãŒ Bedrock API ã‚’å‘¼ã°ãªã„ã‚ˆã†ã€`_call_bedrock_haiku` ã‚’ãƒ¢ãƒƒã‚¯ã™ã‚‹ã€‚ã‚¯ãƒ©ã‚¹ãƒ¬ãƒ™ãƒ«ã®ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã§ä¸€æ‹¬é©ç”¨:

```python
@patch("tools.bet_proposal._call_bedrock_haiku", return_value=None)
class TestProposalReasoning:
    """_generate_proposal_reasoning ã®ãƒ†ã‚¹ãƒˆ.

    _call_bedrock_haiku ã‚’ None ã§è¿”ã™ãƒ¢ãƒƒã‚¯ã«ã™ã‚‹ã“ã¨ã§ã€
    å¸¸ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆï¼‰ãŒä½¿ã‚ã‚Œã‚‹ã€‚
    æ—¢å­˜ãƒ†ã‚¹ãƒˆã®æœŸå¾…å€¤ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆã®å‡ºåŠ›ã«åˆè‡´ã—ã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾ç¶­æŒã€‚
    """

    def _make_reasoning_args(self, *, skip_score: int = 3, preferred_bet_types=None):
        # ... æ—¢å­˜ã®ã¾ã¾

    def test_ææ¡ˆæ ¹æ‹ ãŒæ–‡å­—åˆ—ã‚’è¿”ã™(self, mock_haiku):
        # ... æ—¢å­˜ã®ã¾ã¾ï¼ˆmock_haiku å¼•æ•°ã‚’è¿½åŠ ã™ã‚‹ã ã‘ï¼‰
```

å„ãƒ†ã‚¹ãƒˆãƒ¡ã‚½ãƒƒãƒ‰ã®ã‚·ã‚°ãƒãƒãƒ£ã« `mock_haiku` å¼•æ•°ã‚’è¿½åŠ ï¼ˆ`@patch` ã‚¯ãƒ©ã‚¹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã«ã‚ˆã‚Šè‡ªå‹•æ³¨å…¥ï¼‰ã€‚ãƒ†ã‚¹ãƒˆã®ä¸­èº«ã¯å¤‰æ›´ä¸è¦ã€‚

**Step 2: `TestProposalReasoningInImpl` ã‚‚åŒæ§˜ã«ãƒ¢ãƒƒã‚¯é©ç”¨**

```python
@patch("tools.bet_proposal._call_bedrock_haiku", return_value=None)
class TestProposalReasoningInImpl:
```

**Step 3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã—ã¦å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸã‚’ç¢ºèª**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py -v --tb=short 2>&1 | tail -30`
Expected: ALL PASS

**Step 4: å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest --tb=short 2>&1 | tail -10`
Expected: ALL PASSï¼ˆ2000ä»¶ä»¥ä¸Šï¼‰

**Step 5: ã‚³ãƒŸãƒƒãƒˆ**

```bash
git add backend/tests/agentcore/test_bet_proposal.py
git commit -m "test: æ—¢å­˜ãƒ†ã‚¹ãƒˆã«Bedrock Haikuãƒ¢ãƒƒã‚¯ã‚’é©ç”¨"
```

---

### Task 7: LLMâ†’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚’è¿½åŠ 

**Files:**
- Test: `backend/tests/agentcore/test_bet_proposal.py`

**Step 1: çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ **

```python
class TestLlmNarrationIntegration:
    """LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³â†’ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®çµ±åˆãƒ†ã‚¹ãƒˆ."""

    def _make_reasoning_args(self):
        runners = _make_runners(6)
        ai_preds = _make_ai_predictions(6)
        return dict(
            axis_horses=[
                {"horse_number": 1, "horse_name": "ãƒ†ã‚¹ãƒˆé¦¬1", "composite_score": 85.0},
            ],
            difficulty={"difficulty_stars": 3, "difficulty_label": "æ¨™æº–"},
            predicted_pace="ãƒŸãƒ‰ãƒ«",
            ai_consensus="æ¦‚ã­åˆæ„",
            skip={"skip_score": 3, "reasons": [], "recommendation": "å‚æˆ¦æ¨å¥¨"},
            bets=[
                {
                    "bet_type": "quinella", "bet_type_name": "é¦¬é€£",
                    "horse_numbers": [1, 3], "expected_value": 1.8,
                    "composite_odds": 8.5, "confidence": "high",
                },
            ],
            preferred_bet_types=None,
            ai_predictions=ai_preds,
            runners_data=runners,
        )

    @patch("tools.bet_proposal._call_bedrock_haiku")
    def test_LLMæˆåŠŸæ™‚ã¯LLMç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆãŒä½¿ã‚ã‚Œã‚‹(self, mock_call):
        """Haikuæ­£å¸¸æ™‚ã¯LLMç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆãŒè¿”ã‚‹."""
        llm_text = (
            "ã€è»¸é¦¬é¸å®šã€‘1ç•ªãƒ†ã‚¹ãƒˆé¦¬1ï¼ˆAIæŒ‡æ•°1ä½ï¼‰ã‚’è»¸ã«æ®ãˆãŸã€‚å‰èµ°ã®èµ°ã‚ŠãŒå®‰å®š\n\n"
            "ã€åˆ¸ç¨®ã€‘é›£æ˜“åº¦â˜…3ã®æ¨™æº–ãƒ¬ãƒ¼ã‚¹ã€‚é¦¬é€£ã§å‹è² \n\n"
            "ã€çµ„ã¿åˆã‚ã›ã€‘ç›¸æ‰‹ã¯3ç•ªãƒ†ã‚¹ãƒˆé¦¬3ã€‚æœŸå¾…å€¤1.8ã¨é«˜ã„\n\n"
            "ã€ãƒªã‚¹ã‚¯ã€‘AIåˆè­°ã€Œæ¦‚ã­åˆæ„ã€ã€‚ç©æ¥µå‚æˆ¦ãƒ¬ãƒ™ãƒ«"
        )
        mock_call.return_value = llm_text
        result = _generate_proposal_reasoning(**self._make_reasoning_args())
        assert "å‰èµ°ã®èµ°ã‚ŠãŒå®‰å®š" in result  # LLMå›ºæœ‰ã®ãƒ†ã‚­ã‚¹ãƒˆ

    @patch("tools.bet_proposal._call_bedrock_haiku")
    def test_LLMå¤±æ•—æ™‚ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã™ã‚‹(self, mock_call):
        """Haiku APIã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç”Ÿæˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯."""
        mock_call.side_effect = Exception("ServiceUnavailable")
        result = _generate_proposal_reasoning(**self._make_reasoning_args())
        assert "ã€è»¸é¦¬é¸å®šã€‘" in result
        assert isinstance(result, str)
        assert len(result) > 0

    @patch("tools.bet_proposal._call_bedrock_haiku")
    def test_LLMãŒä¸å®Œå…¨ãªå‡ºåŠ›ã‚’ã—ãŸå ´åˆã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯(self, mock_call):
        """4ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒã‚ãªã„å ´åˆã¯ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯."""
        mock_call.return_value = "ä¸­é€”åŠç«¯ãªå›ç­”"
        result = _generate_proposal_reasoning(**self._make_reasoning_args())
        assert "ã€è»¸é¦¬é¸å®šã€‘" in result
        assert "ã€ãƒªã‚¹ã‚¯ã€‘" in result
```

**Step 2: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest tests/agentcore/test_bet_proposal.py::TestLlmNarrationIntegration -v`
Expected: ALL PASS

**Step 3: å…¨ãƒ†ã‚¹ãƒˆæœ€çµ‚ç¢ºèª**

Run: `cd /home/inoue-d/dev/baken-kaigi/llm-narration/backend && uv run pytest --tb=short 2>&1 | tail -10`
Expected: ALL PASS

**Step 4: ã‚³ãƒŸãƒƒãƒˆ**

```bash
git add backend/tests/agentcore/test_bet_proposal.py
git commit -m "test: LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ±åˆãƒ†ã‚¹ãƒˆã‚’è¿½åŠ "
```

---

### Task 8: è¨­è¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ›´æ–°ã—ã¦PRä½œæˆ

**Files:**
- Modify: `docs/plans/2026-02-14-llm-narration-design.md`ï¼ˆå®Ÿè£…å®Œäº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã«æ›´æ–°ï¼‰

**Step 1: PRä½œæˆ**

```bash
git push -u origin feature/llm-narration
gh pr create --title "feat: è²·ã„ç›®ææ¡ˆã®æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆã‚’LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ç½®æ›" --body "$(cat <<'EOF'
## Summary
- è²·ã„ç›®ææ¡ˆã®æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆï¼ˆproposal_reasoningï¼‰ã‚’Haiku 4.5ã«ã‚ˆã‚‹è‡ªç„¶è¨€èªç”Ÿæˆã«ç½®æ›
- Phase 0-6ã®æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã¯ä¸€åˆ‡å¤‰æ›´ãªã—
- Bedrock APIå¤±æ•—æ™‚ã¯æ—¢å­˜ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯

## å¤‰æ›´å†…å®¹
- `_build_narration_context()`: LLMç”¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
- `_invoke_haiku_narrator()`: Bedrock Converse APIå‘¼ã³å‡ºã—
- `_generate_proposal_reasoning()`: LLMå„ªå…ˆãƒ»ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- æ—¢å­˜ãƒ†ã‚¹ãƒˆã«Bedrockãƒ¢ãƒƒã‚¯é©ç”¨ã€çµ±åˆãƒ†ã‚¹ãƒˆè¿½åŠ 

## Test plan
- [ ] å…¨æ—¢å­˜ãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹ã™ã‚‹ã“ã¨
- [ ] LLMãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³é–¢é€£ã®æ–°è¦ãƒ†ã‚¹ãƒˆãŒãƒ‘ã‚¹ã™ã‚‹ã“ã¨
- [ ] æœ¬ç•ªç’°å¢ƒã§ææ¡ˆå®Ÿè¡Œã—ã€æ ¹æ‹ ãƒ†ã‚­ã‚¹ãƒˆãŒè‡ªç„¶è¨€èªã§ç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª

ğŸ¤– Generated with [Claude Code](https://claude.com/claude-code)
EOF
)"
```
